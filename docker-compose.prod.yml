# Docker Compose for LiquidCrypto Production - Blue-Green Deployment
# Run: docker compose -f docker-compose.prod.yml up -d

services:
  # Blue backend (primary)
  backend-blue:
    image: ghcr.io/marioberlin/liquidcrypto-backend:${BLUE_VERSION:-latest}
    container_name: liquidcrypto-backend-blue
    env_file: .env.production
    environment:
      - PORT=3000
      - NATS_URL=nats://nats:4222
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      nats:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://localhost:3000/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Green backend (secondary)
  backend-green:
    image: ghcr.io/marioberlin/liquidcrypto-backend:${GREEN_VERSION:-latest}
    container_name: liquidcrypto-backend-green
    env_file: .env.production
    environment:
      - PORT=3000
      - NATS_URL=nats://nats:4222
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      nats:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://localhost:3000/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # NATS: High-performance messaging for A2A agents and real-time events
  nats:
    image: nats:2.10-alpine
    container_name: liquidcrypto-nats
    command: [ "--js", "--sd", "/data", "-m", "8222" ]
    restart: unless-stopped
    volumes:
      - nats_data:/data
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider", "http://localhost:8222/healthz" ]
      interval: 5s
      timeout: 3s
      retries: 3

  postgres:
    image: postgres:16-alpine
    container_name: liquidcrypto-postgres
    environment:
      POSTGRES_USER: liquidcrypto
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-liquidcrypto_prod}
      POSTGRES_DB: liquidcrypto
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./server/sql:/docker-entrypoint-initdb.d:ro
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U liquidcrypto -d liquidcrypto" ]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    container_name: liquidcrypto-redis
    volumes:
      - redisdata:/data
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "redis-cli", "ping" ]
      interval: 5s
      timeout: 5s
      retries: 5

  # AI Agent Runtime Container - always running for agent sandboxes
  liquid-runtime:
    image: ghcr.io/marioberlin/liquidcrypto-runtime:${RUNTIME_VERSION:-latest}
    container_name: liquidcrypto-runtime
    restart: always
    volumes:
      - liquid_sandboxes:/sandboxes:rw
    environment:
      LIQUID_RUNTIME_MODE: pool
      LIQUID_RUNTIME_PORT: 8080
      LIQUID_MAX_EXECUTION_TIME: 300000
      LIQUID_MAX_MEMORY_MB: 512
      LIQUID_LOG_LEVEL: info
    networks:
      - liquid-network
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s

  # Video rendering service
  video-runtime:
    image: ghcr.io/marioberlin/liquidcrypto-video:${VIDEO_VERSION:-latest}
    container_name: liquidcrypto-video
    environment:
      - VIDEO_PORT=8082
      - VIDEO_OUTPUT_DIR=/data/renders
      - VIDEO_ASSET_DIR=/data/assets
      - VIDEO_TEMP_DIR=/data/temp
      - VIDEO_MAX_CONCURRENT_RENDERS=4
      - VIDEO_DEFAULT_CODEC=h264
      - VIDEO_DEFAULT_CRF=18
    volumes:
      - video_renders:/data/renders
      - video_assets:/data/assets
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "curl", "-sf", "http://localhost:8082/health" ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s

  # Frontend: Caddy + static files as immutable Docker image
  # This replaces the old caddy service that mounted ./dist as a volume
  frontend:
    image: ghcr.io/marioberlin/liquidcrypto-frontend:${FRONTEND_VERSION:-latest}
    container_name: liquidcrypto-frontend
    ports:
      - "80:80"
      - "443:443"
    volumes:
      # Caddyfile is baked into image, but can be overridden if needed
      - ./Caddyfile:/etc/caddy/Caddyfile:ro
      - ./active-backend:/etc/caddy/active-backend:ro
      - caddy_data:/data
      - caddy_config:/config
      # Note: ./dist volume removed - frontend is now IN the image
    depends_on:
      - backend-blue
      - backend-green
      - video-runtime
    restart: unless-stopped
    networks:
      - liquid-network
    healthcheck:
      test: [ "CMD", "wget", "-q", "--spider", "http://localhost/health" ]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s

networks:
  liquid-network:
    driver: bridge

volumes:
  pgdata:
  redisdata:
  nats_data:
  caddy_data:
  caddy_config:
  liquid_sandboxes:
  video_renders:
  video_assets:
